\relax 
\citation{ben2007analysis}
\citation{blitzer2008learning}
\citation{lowe1999object}
\citation{bay2006surf}
\citation{dalal2005histograms}
\citation{krizhevsky2012imagenet}
\citation{zeiler2014visualizing}
\citation{simonyan2014very}
\citation{farabet2013learning}
\citation{zeiler2014visualizing}
\citation{Chatfield14}
\citation{hoffman2013one}
\citation{NIPS2014_Zhou}
\citation{srivastava2014dropout}
\citation{zhang2014part}
\citation{daume2009frustratingly}
\citation{yang2007adapting}
\citation{aytar2011tabula}
\citation{aytar2011tabula}
\citation{yildirim2002warm}
\citation{john2008implementation}
\citation{zeilinger2011real}
\citation{chuwarm}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\newlabel{sec:intro}{{I}{1}}
\citation{bossard2014food}
\citation{kawano14c}
\citation{bossard2014food}
\citation{zeiler2014visualizing}
\citation{simonyan2014very}
\citation{krizhevsky2012imagenet}
\citation{zeiler2010deconvolutional}
\citation{CiresanIJCAI11}
\citation{krizhevsky2012imagenet}
\citation{jia2014caffe}
\citation{szegedy2014going}
\@writefile{toc}{\contentsline {section}{\numberline {II}Background}{2}}
\newlabel{sec:bg}{{II}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Food Datasets}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Challenges of food recognition task}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Training Deep Feature Extractor}{2}}
\newlabel{sec:ft}{{III}{2}}
\citation{linNiN}
\citation{szegedy2014going}
\citation{krizhevsky2012imagenet}
\citation{agrawal2014analyzing}
\citation{glorot2010understanding}
\citation{bossard2014food}
\citation{singh2012unsupervised}
\citation{farabet2013learning}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The classifier for class $j$ is initialized with $\mathaccent "705E\relax {f}$ and converged at $f_j$. (a) Conventional adaptation method using the parameters of the learned category and need more training time find good solution for the target category. (b) The parameters from $\mathaccent "705E\relax {f}$ reject all learned categories and can learn faster for the new categories.}}{3}}
\newlabel{fig:wm}{{1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Inception Module. $n\times n$ stands for size $n$ receptive field, $n\times n\_reduce$ stands for the $1\times 1$ convolutional layer before the $n\times n$ convolution layer and $pool\_proj$ is another $1\times 1$ convolutional layer after the MAX pooling layer. The output layer concatenates all its input layers.}}{3}}
\newlabel{incept}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Architecture of GoogLeNet}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Pre-training and Fine-tuning GoogLeNet}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Top-5 Accuracy for different deep cnn architectures}}{3}}
\newlabel{tab:ft}{{I}{3}}
\citation{zhang2014part}
\citation{razavian2014cnn}
\citation{krizhevsky2012imagenet}
\citation{simonyan2014very}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Top-1 accuracy compared to other methods on Food-101 dataset in percent}}{4}}
\newlabel{tab:101}{{II}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Accuracy for different number of examples in each category}}{4}}
\newlabel{tab:mini}{{III}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Visualization of some feature maps of GoogLeNet models in different layers for the same input image. 64 feature maps of each layer are shown. Conv1 is the first convolutional layer and Inception\_5b is the last convolutional layer. }}{4}}
\newlabel{fig:sashimi}{{3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Advantage of GoogLeNet for fine-tuning}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Cosine similarity of the weights for similar foods}}{4}}
\newlabel{tab:weight_sim}{{IV}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces Average cosine similarity of the representation for similar foods}}{4}}
\newlabel{tab:pre_sim}{{V}{4}}
\citation{glorot2010understanding}
\citation{NairH10}
\citation{Chatfield14}
\citation{zeiler2014visualizing}
\citation{hoffman2013one}
\citation{srivastava2014dropout}
\citation{yang2007adapting}
\citation{aytar2011tabula}
\citation{aytar2011tabula}
\citation{aytar2011tabula}
\citation{chuwarm}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Warm Start Domain Adaptation for Learning New Categories}{5}}
\newlabel{sec:da}{{IV}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Limitation of previous approaches}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Incremental adaptive learning for new category}{5}}
\citation{hoffman2013one}
\@writefile{lot}{\contentsline {table}{\numberline {VI}{\ignorespaces Cosine similarity of the layers in inception modules between fine-tuned models and pre-trained model for GoogLeNet}}{6}}
\newlabel{tab:cosg}{{VI}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {VII}{\ignorespaces Cosine similarity of the layers between fine-tuned models and pre-trained model for AlexNet}}{6}}
\newlabel{tab:cosa}{{VII}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {VIII}{\ignorespaces Average Precision for A-SVM, PMT-SVM, SVM and Logistic Regression trained with deep representation. Source categories without * are determined by cosine similarity}}{6}}
\newlabel{tab:su_domian}{{VIII}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}Experiment setup \& Evaluation}{6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Warm start adaptation with negative classifier}}{6}}
\newlabel{algo:ws}{{1}{6}}
\citation{jia2014caffe}
\bibstyle{IEEEtran}
\bibdata{IEEEabrv,research}
\bibcite{ben2007analysis}{1}
\bibcite{blitzer2008learning}{2}
\bibcite{lowe1999object}{3}
\bibcite{bay2006surf}{4}
\bibcite{dalal2005histograms}{5}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-D}}Adapting single new categories}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {IX}{\ignorespaces Some results of top-5 Accuracy for adapting single new category.}}{7}}
\newlabel{tab:N+1}{{IX}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-E}}Adapting multiple new categories}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {X}{\ignorespaces Overall top-5 accuracy for adapted categories in learned categories with different training iteration in each step.}}{7}}
\newlabel{tab:it}{{X}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{7}}
\@writefile{toc}{\contentsline {section}{References}{7}}
\bibcite{krizhevsky2012imagenet}{6}
\bibcite{zeiler2014visualizing}{7}
\bibcite{simonyan2014very}{8}
\bibcite{farabet2013learning}{9}
\bibcite{Chatfield14}{10}
\bibcite{hoffman2013one}{11}
\bibcite{NIPS2014_Zhou}{12}
\bibcite{srivastava2014dropout}{13}
\bibcite{zhang2014part}{14}
\bibcite{daume2009frustratingly}{15}
\bibcite{yang2007adapting}{16}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Top-5 accuracy curve for categories in Food-220 in super-domain with 5 shots. Mean and standard deviation are shown.}}{8}}
\newlabel{fig:wama}{{4}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Overall learning curve in some steps for 300 iterations. \textbf  {Observation:} even training with enough iteration, warm start can still outperform cold start in a single step.}}{8}}
\newlabel{fig:errdiff}{{5}{8}}
\bibcite{aytar2011tabula}{17}
\bibcite{yildirim2002warm}{18}
\bibcite{john2008implementation}{19}
\bibcite{zeilinger2011real}{20}
\bibcite{chuwarm}{21}
\bibcite{bossard2014food}{22}
\bibcite{kawano14c}{23}
\bibcite{zeiler2010deconvolutional}{24}
\bibcite{CiresanIJCAI11}{25}
\bibcite{jia2014caffe}{26}
\bibcite{szegedy2014going}{27}
\bibcite{linNiN}{28}
\bibcite{agrawal2014analyzing}{29}
\bibcite{glorot2010understanding}{30}
\bibcite{singh2012unsupervised}{31}
\bibcite{razavian2014cnn}{32}
\bibcite{NairH10}{33}
