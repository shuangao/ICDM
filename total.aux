\relax 
\citation{lowe1999object}
\citation{bay2006surf}
\citation{dalal2005histograms}
\citation{krizhevsky2012imagenet}
\citation{zeiler2014visualizing}
\citation{simonyan2014very}
\citation{szegedy2014going}
\citation{farabet2013learning}
\citation{ben2007analysis}
\citation{blitzer2008learning}
\citation{daume2009frustratingly}
\citation{yang2007adapting}
\citation{aytar2011tabula}
\citation{Chatfield14}
\citation{zeiler2014visualizing}
\citation{hoffman2013one}
\citation{NIPS2014_Zhou}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\newlabel{sec:intro}{{I}{1}}
\citation{kawano14c}
\citation{bossard14}
\citation{zeiler2010deconvolutional}
\citation{krizhevsky2012imagenet}
\citation{CiresanIJCAI11}
\citation{krizhevsky2012imagenet}
\citation{linNiN}
\citation{szegedy2014going}
\citation{agrawal2014analyzing}
\@writefile{toc}{\contentsline {section}{\numberline {II}Background}{2}}
\newlabel{sec:bg}{{II}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Food Datasets}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Challenges of food recognition task}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Inception Module. $n\times n$ stands for size $n$ receptive field, $n\times n\_reduce$ stands for the $1\times 1$ convolutional layer before the $n\times n$ convolution layer and $pool\_proj$ is another $1\times 1$ convolutional layer after the MAX pooling layer. The output layer concatenates all its input layers.}}{2}}
\newlabel{incept}{{1}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Deep Feature Extractor}{2}}
\newlabel{sec:ft}{{III}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Architecture of GoogLeNet}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Pre-training and Fine-tuning GoogLeNet}{2}}
\citation{glorot2010understanding}
\citation{bossard14}
\citation{singh2012unsupervised}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Top-5 Accuracy for different deep cnn architectures}}{3}}
\newlabel{tab:ft}{{I}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Discussion of the unique architecture of GoogLeNet}{3}}
\newlabel{relu}{{1}{3}}
\citation{Chatfield14}
\citation{zeiler2014visualizing}
\citation{hoffman2013one}
\citation{yang2007adapting}
\citation{aytar2011tabula}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Top-1 accuracy compared to other methods on Food-101 dataset in percent}}{4}}
\newlabel{tab:101}{{II}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Visualization of some feature maps of GoogLeNet models in different layers for the same input image. 64 feature maps of each layer are shown. Conv1 is the first convolutional layer and Inception\_5b is the last convolutional layer. }}{4}}
\newlabel{fig:sashimi}{{2}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Cosine similarity of the layers in inception modules between fine-tuned models and pre-trained model for GoogLeNet}}{4}}
\newlabel{tab:cosg}{{III}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Cosine similarity of the layers between fine-tuned models and pre-trained model for AlexNet}}{4}}
\newlabel{tab:cosa}{{IV}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Warm Start Domain Adaptation for Learning New Categories}{4}}
\newlabel{sec:da}{{IV}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Limits of previous approaches}{4}}
\citation{chuwarm}
\citation{hoffman2013one}
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces Sparsity of the output for each unit in GoogLeNet inception module for training data from Food101 in percent}}{5}}
\newlabel{tab:sparse}{{V}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Incremental adaptive learning for new category}{5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Complete algorithm of warm start adaptation}}{5}}
\newlabel{algo:ws}{{1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}Experiment setup \& Evaluation}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {VI}{\ignorespaces Average Precision for A-SVM, PMT-SVM, SVM and Logistic Regression. Source categories without * are determined by cosine similarity}}{6}}
\newlabel{tab:su_domian}{{VI}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces (a) Conventional adaptation method fail to find good initialization for the new category from source domain. (b) The parameters from $\mathaccent "705E\relax {f}$ are more adaptive for the new categories.}}{6}}
\newlabel{fig:wm}{{3}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-D}}From 101 to 102 categories}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {VII}{\ignorespaces Accuracy for a single new target category in M+1 experiment. Average top-5 accuracies for some categories are shown. Last row shows the average results for all categories.}}{6}}
\newlabel{tab:N+1}{{VII}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-E}}From 101 to 101+220 categories}{6}}
\bibstyle{IEEEtran}
\bibdata{IEEEabrv,research}
\bibcite{lowe1999object}{1}
\bibcite{bay2006surf}{2}
\bibcite{dalal2005histograms}{3}
\bibcite{krizhevsky2012imagenet}{4}
\bibcite{zeiler2014visualizing}{5}
\bibcite{simonyan2014very}{6}
\bibcite{szegedy2014going}{7}
\bibcite{farabet2013learning}{8}
\bibcite{ben2007analysis}{9}
\bibcite{blitzer2008learning}{10}
\bibcite{daume2009frustratingly}{11}
\bibcite{yang2007adapting}{12}
\bibcite{aytar2011tabula}{13}
\bibcite{Chatfield14}{14}
\bibcite{hoffman2013one}{15}
\bibcite{NIPS2014_Zhou}{16}
\bibcite{kawano14c}{17}
\bibcite{bossard14}{18}
\bibcite{zeiler2010deconvolutional}{19}
\bibcite{CiresanIJCAI11}{20}
\bibcite{linNiN}{21}
\bibcite{agrawal2014analyzing}{22}
\bibcite{glorot2010understanding}{23}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{7}}
\@writefile{toc}{\contentsline {section}{References}{7}}
\bibcite{singh2012unsupervised}{24}
\bibcite{chuwarm}{25}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Top-5 accuracy curve for categories in Food-256 in super-domain with 5 shots. Mean and standard deviation are shown.}}{8}}
\newlabel{fig:wama}{{4}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Overall learning curve in some steps for 300 iterations. \textbf  {Observation:} even training with enough iteration, warm start can still outperform cold start in a single step.}}{8}}
\newlabel{fig:errdiff}{{5}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {VIII}{\ignorespaces Overall top-5 accuracy for 220 new categories in super-domain with different training iteration in each step.}}{8}}
\newlabel{tab:it}{{VIII}{8}}
