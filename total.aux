\relax 
\citation{lowe1999object}
\citation{bay2006surf}
\citation{dalal2005histograms}
\citation{krizhevsky2012imagenet}
\citation{zeiler2014visualizing}
\citation{simonyan2014very}
\citation{szegedy2014going}
\citation{farabet2013learning}
\citation{ben2007analysis}
\citation{blitzer2008learning}
\citation{daume2009frustratingly}
\citation{yang2007adapting}
\citation{aytar2011tabula}
\citation{Chatfield14}
\citation{zeiler2014visualizing}
\citation{hoffman2013one}
\citation{NIPS2014_Zhou}
\citation{kawano14c}
\citation{bossard14}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\newlabel{sec:intro}{{I}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Background}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Food Datasets}{1}}
\citation{zeiler2010deconvolutional}
\citation{krizhevsky2012imagenet}
\citation{linNiN}
\citation{szegedy2014going}
\citation{agrawal2014analyzing}
\citation{glorot2010understanding}
\citation{Kawano:2014}
\citation{bossard14}
\citation{singh2012unsupervised}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Deep Convolutional Neural Network}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Training Deep Feature Extractor}{2}}
\newlabel{sec:ft}{{III}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Architecture of GoogLeNet}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Inception Module. $n\times n$ stands for size $n$ receptive field, $n\times n\_reduce$ stands for the $1\times 1$ convolutional layer before the $n\times n$ convolution layer and $pool\_proj$ is another $1\times 1$ convolutional layer after the MAX pooling layer. The output layer concatenates all its input layers.}}{2}}
\newlabel{incept}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Pre-training and Fine-tuning}{2}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Top-5 Accuracy in percent on fine-tuned, ft-last and scratch model for two architectures}}{2}}
\newlabel{tab:ft}{{I}{2}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Top-1 accuracy compared to other methods on Food-101 dataset in percent}}{3}}
\newlabel{tab:101}{{III}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Visualization of some feature maps of different GoogLeNet models in different layers for the same input image. 64 feature maps of each layer are shown. Conv1 is the first convolutional layer and Inception\_5b is the last convolutional layer. }}{3}}
\newlabel{fig:sashimi}{{2}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Accuracy compared to other method on Food-256 dataset in percent}}{3}}
\newlabel{tab:256}{{II}{3}}
\newlabel{relu}{{1}{3}}
\citation{Chatfield14}
\citation{zeiler2014visualizing}
\citation{hoffman2013one}
\citation{gong2012geodesic}
\citation{fernando2013unsupervised}
\citation{yang2007adapting}
\citation{aytar2011tabula}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Cosine similarity of the layers in inception modules between fine-tuned models and pre-trained model for GoogLeNet}}{4}}
\newlabel{tab:cosg}{{IV}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces Cosine similarity of the layers between fine-tuned models and pre-trained model for AlexNet}}{4}}
\newlabel{tab:cosa}{{V}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {VI}{\ignorespaces Sparsity of the output for each unit in GoogLeNet inception module for training data from Food101 in percent}}{4}}
\newlabel{tab:sparse}{{VI}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Domain Adaptation}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Limits of previous approaches}{4}}
\citation{chuwarm}
\bibstyle{IEEEtran}
\bibdata{IEEEabrv,research}
\bibcite{lowe1999object}{1}
\bibcite{bay2006surf}{2}
\bibcite{dalal2005histograms}{3}
\bibcite{krizhevsky2012imagenet}{4}
\bibcite{zeiler2014visualizing}{5}
\bibcite{simonyan2014very}{6}
\bibcite{szegedy2014going}{7}
\bibcite{farabet2013learning}{8}
\bibcite{ben2007analysis}{9}
\bibcite{blitzer2008learning}{10}
\bibcite{daume2009frustratingly}{11}
\bibcite{yang2007adapting}{12}
\bibcite{aytar2011tabula}{13}
\bibcite{Chatfield14}{14}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}our method}{5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Warm start online adaptation}}{5}}
\newlabel{algo:ws}{{1}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{5}}
\@writefile{toc}{\contentsline {section}{References}{5}}
\bibcite{hoffman2013one}{15}
\bibcite{NIPS2014_Zhou}{16}
\bibcite{kawano14c}{17}
\bibcite{bossard14}{18}
\bibcite{zeiler2010deconvolutional}{19}
\bibcite{linNiN}{20}
\bibcite{agrawal2014analyzing}{21}
\bibcite{glorot2010understanding}{22}
\bibcite{Kawano:2014}{23}
\bibcite{singh2012unsupervised}{24}
\bibcite{gong2012geodesic}{25}
\bibcite{fernando2013unsupervised}{26}
\bibcite{chuwarm}{27}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces ASVM vs Ap}}{6}}
