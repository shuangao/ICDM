Learning to classify new categories from different domains is always an interesting and challenging topic in data mining.
Previous empirical and theoretical studies have shown that the testing error is positively proportional to the difference between the test and training input distributions \cite{ben2007analysis} \cite{blitzer2008learning}. Therefore, mismatched data distribution can be a hug problem for predicting the data from a different domain. Domain adaptation is proposed to solve this problem, transferring the knowledge from the source domain to target ones. For image recognition tasks, the model trained from the source domain is inherently biased as there is no database that includes all the transformations of the images. Many supervised domain adaptation methods for the image recognition task have been proposed to compensate this bias with shallow feature representations, assuming that there should be a linear correlation of the parameters for similar categories from two domains \cite{daume2009frustratingly} \cite{yang2007adapting} \cite{aytar2011tabula}. However, this assumption could fail when the algorithm try to adapt less similar categories.

Recently, Convolutional Neural Network (CNN) shows its potential to replace the shallow features, such as SIFT \cite{lowe1999object}, SURF \cite{bay2006surf} and HOG \cite{dalal2005histograms} etc, in large object recognition tasks \cite{krizhevsky2012imagenet} \cite{zeiler2014visualizing} \cite{simonyan2014very}. Unlike the local feature which presents an shallow interpretation of spatial property, deep CNN can automatically learn top-down hierarchical feature representations. Therefore, deep CNN has been intensively used as the feature extractor for image recognition \cite{farabet2013learning}. As deep feature representation has strong generalization ability, it can compensate for the data bias and be applied for domain adaptation. Fine-tuning the whole network with the pre-trained models on target domain directly has shown some impressive results from previous studies \cite{Chatfield14} \cite{zeiler2014visualizing} \cite{hoffman2013one} \cite{NIPS2014_Zhou}.
However, fine-tuning large deep CNN with limited training data could lead to overfitting which is mostly due to the sampling noise \cite{srivastava2014dropout}.

Since the previous methods are limited to either similar categories or requiring many labeled examples, it would be ideal for a method that can learn and predict new categories with just a few labeled training examples. Even though, it is unlikely to obtain a satisfied result while fine-tuning the whole network for deep CNN on a small target domain dataset, fine-tuned deep CNN on a relatively large source domain set can still extract fine-grained feature representation that can alleviate the distribution bias for target domain \cite{zhang2014part}. Moreover, our empirical experiments show that fine-grained deep representation can also help classifier learn categories with fewer examples.

The previous adaptation methods require the target category to be geometrically similar to source category \cite{aytar2011tabula}. So their shallow feature representations should be similar and the linear assumption of the parameters can be applied directly for adaptation.
However, deep CNN can learn fine-grained feature representations. The feature representations of two similar categories are not necessarily to be close to each other and the linear correlation assumption may fail for deep representation. Therefore, the parameters of classifiers for different categories should be more dissimilar for deep representation compared to shallow ones. Alternatively we employ a negative classifier that rejects all learned categories and assume that warm start the new classifier with parameters different from the negative classifiers can achieve better result.
The idea of "warm start" has been widely used for linear optimization problem, where the algorithm iteratively initializes and updates the parameters from the result of previous steps \cite{yildirim2002warm} \cite{john2008implementation} \cite{zeilinger2011real} \cite{chuwarm}. In our work, the warm start is used to initialize the classifier for the new category with the parameters from a pre-trained negative classifier that predicts all learned categories as negative examples.

 In this paper, we propose an incremental adaptive approach with warm start technique to learn new categories for food image recognition. There are two main contributions of our work:
\begin{enumerate}
  \item Training deep feature extractor. To generate fine-grained deep representation from images, we first train the efficient feature extractor with fine-tuned GoogLeNet and achieve the state-of-the-art performance on our source domain, Food-101 dataset. Compared to the results training on full dataset, we can achieve similar results while just using a few examples in each category with deep representation.
  \item Negative classifier for warm start. We find that previous domain adaptation methods suffer as the difference between target and source domain increases. Warm start new classifier with negative classifier could take advantage of deep representation. Benefitting from warm start parameters in each step, empirical experiments show that out method can achieve better result compared to cold start and warm start parameters converges better with a few training iterations compared to cold start as well.
\end{enumerate}

The rest of this paper is organized as follow: In Section \ref{sec:bg} we introduce the two datasets and discuss some properties about the food recognition task. In order to obtain discriminative representations from images, we fine-tuned GoogLeNet with pre-trained parameters from ImageNet and achieve state-of-the-art performance on our source domain, Food-101 dataset in Section \ref{sec:ft}. We discuss the limitations of some previous adaptation methods and propose our warm start adaptation method for learning new categories in Section \ref{sec:da}.
